{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MOdel selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model hyper parameter tuning\n",
    "from sklearn import metrics\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score,roc_curve\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(folder, samp = False, lof=False):\n",
    "    df = pd.DataFrame()\n",
    "    path = folder+'/'\n",
    "    df_new = pd.read_csv(path+'benign_traffic.csv')\n",
    "    df_new['label'] = 'benign_traffic'\n",
    "    if samp==True:\n",
    "        df_new = df_new.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "    if lof==True:\n",
    "        #df_new = pd.DataFrame(scaler.fit_transform(df_new))\n",
    "        df_new = filter_lof(df_new)\n",
    "        #df_new = scaler.inverse_transform(df_new)\n",
    "    df = pd.concat([df,df_new], ignore_index=True)\n",
    "    \n",
    "    directory = os.fsencode(path+'gafgyt_attacks')\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".csv\"): \n",
    "            df_new = pd.read_csv(path+'gafgyt_attacks'+'/'+filename)\n",
    "            filename=filename[:-4] \n",
    "            df_new['label'] = 'gafgyt_'+filename\n",
    "            if samp==True:\n",
    "                df_new = df_new.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "            if lof==True:\n",
    "                #df_new = pd.DataFrame(scaler.fit_transform(df_new))\n",
    "                df_new = filter_lof(df_new)\n",
    "                #df_new = scaler.inverse_transform(df_new)\n",
    "            df = pd.concat([df,df_new], ignore_index=True)\n",
    "    directory = os.fsencode(path+'mirai_attacks')\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".csv\"): \n",
    "            df_new = pd.read_csv(path+'mirai_attacks'+'/'+filename)\n",
    "            filename=filename[:-4] \n",
    "            df_new['label'] = 'mirai_'+filename\n",
    "            if samp==True:\n",
    "                df_new = df_new.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "            if lof==True:\n",
    "                #df_new = pd.DataFrame(scaler.fit_transform(df_new))\n",
    "                df_new = filter_lof(df_new)\n",
    "                #df_new = scaler.inverse_transform(df_new)\n",
    "            df = pd.concat([df,df_new], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lof(df, k=20) :\n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    scaler = StandardScaler()\n",
    "    df2 = pd.DataFrame.copy(df)\n",
    "    df2 = df2.drop(columns = ['label'])\n",
    "    df2 = pd.DataFrame(scaler.fit_transform(df2))\n",
    "    df2[\"_lof\"] = lof.fit_predict(df2)\n",
    "    df2 = df2[df2[\"_lof\"]>0].drop(columns=\"_lof\")#.reset_index(drop=True)\n",
    "    df2 = pd.DataFrame(scaler.inverse_transform(df2))\n",
    "    df2['label'] = df['label']\n",
    "    return df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def churn_prediction(algorithm,training_x,testing_x,\n",
    "                             training_y,testing_y,cols,cf,threshold_plot) :\n",
    "    \n",
    "    #model\n",
    "    algorithm.fit(training_x,training_y)\n",
    "    predictions   = algorithm.predict(testing_x)\n",
    "    probabilities = algorithm.predict_proba(testing_x)\n",
    "    #coeffs\n",
    "    if   cf == \"coefficients\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n",
    "    elif cf == \"features\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n",
    "        \n",
    "    column_df     = pd.DataFrame(cols)\n",
    "    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n",
    "                              right_index= True, how = \"left\"))\n",
    "    coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "    \n",
    "    #print (algorithm)\n",
    "    #print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n",
    "    #print (\"Accuracy   Score : \",accuracy_score(testing_y,predictions))\n",
    "    return accuracy_score(testing_y,predictions)\n",
    "    #confusion matrix\n",
    "    conf_matrix = confusion_matrix(testing_y,predictions)\n",
    "    '''\n",
    "    #roc_auc_score\n",
    "    model_roc_auc = roc_auc_score(testing_y,predictions) \n",
    "    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n",
    "    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n",
    "    #plot confusion matrix\n",
    "    trace1 = go.Heatmap(z = conf_matrix ,\n",
    "                        x = [\"Not churn\",\"Churn\"],\n",
    "                        y = [\"Not churn\",\"Churn\"],\n",
    "                        showscale  = False,colorscale = \"Picnic\",\n",
    "                        name = \"matrix\")\n",
    "    \n",
    "    #plot roc curve\n",
    "    trace2 = go.Scatter(x = fpr,y = tpr,\n",
    "                        name = \"Roc : \" + str(model_roc_auc),\n",
    "                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n",
    "    trace3 = go.Scatter(x = [0,1],y=[0,1],\n",
    "                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n",
    "                        dash = 'dot'))\n",
    "    \n",
    "    #plot coeffs\n",
    "    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n",
    "                    name = \"coefficients\",\n",
    "                    marker = dict(color = coef_sumry[\"coefficients\"],\n",
    "                                  colorscale = \"Picnic\",\n",
    "                                  line = dict(width = .6,color = \"black\")))\n",
    "    \n",
    "    #subplots\n",
    "    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                            'Receiver operating characteristic',\n",
    "                                            'Feature Importances'))\n",
    "    \n",
    "    fig.append_trace(trace1,1,1)\n",
    "    fig.append_trace(trace2,1,2)\n",
    "    fig.append_trace(trace3,1,2)\n",
    "    fig.append_trace(trace4,2,1)\n",
    "    \n",
    "    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n",
    "                         autosize = False,height = 900,width = 800,\n",
    "                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         margin = dict(b = 195))\n",
    "    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n",
    "    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n",
    "    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n",
    "                                        tickangle = 90))\n",
    "    py.iplot(fig)\n",
    "    \n",
    "    if threshold_plot == True : \n",
    "        visualizer = DiscriminationThreshold(algorithm)\n",
    "        visualizer.fit(training_x,training_y)\n",
    "        visualizer.poof()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(filename, algo, seed) :\n",
    "    X, Y = readXY(filename)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "    rfe_algo = algo\n",
    "    \n",
    "    accuracy = churn_prediction(rfe_algo, x_train, x_test, y_train, y_test, x_train.columns,\"features\",threshold_plot = False)\n",
    "    return rfe_algo, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXY(filename) :\n",
    "    df = read_file(filename, samp=True, lof=False)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    X = df.drop(columns=['label'], axis=1)\n",
    "    Y = df[['label']]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def churn_validation(algorithm,testing_x,\n",
    "                             testing_y,cols) :\n",
    "    #model\n",
    "    predictions   = algorithm.predict(testing_x)\n",
    "    probabilities = algorithm.predict_proba(testing_x)\n",
    "    \n",
    "    #print (algorithm)\n",
    "    #print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n",
    "    #print (\"Accuracy   Score : \",accuracy_score(testing_y,predictions))\n",
    "    return accuracy_score(testing_y,predictions)\n",
    "    #confusion matrix\n",
    "    #conf_matrix = confusion_matrix(testing_y,predictions)\n",
    "    '''\n",
    "    #roc_auc_score\n",
    "    model_roc_auc = roc_auc_score(testing_y,predictions) \n",
    "    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n",
    "    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n",
    "    \n",
    "    #plot confusion matrix\n",
    "    trace1 = go.Heatmap(z = conf_matrix ,\n",
    "                        x = [\"Not churn\",\"Churn\"],\n",
    "                        y = [\"Not churn\",\"Churn\"],\n",
    "                        showscale  = False,colorscale = \"Picnic\",\n",
    "                        name = \"matrix\")\n",
    "    \n",
    "    #plot roc curve\n",
    "    trace2 = go.Scatter(x = fpr,y = tpr,\n",
    "                        name = \"Roc : \" + str(model_roc_auc),\n",
    "                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n",
    "    trace3 = go.Scatter(x = [0,1],y=[0,1],\n",
    "                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n",
    "                        dash = 'dot'))\n",
    "    #subplots\n",
    "    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                            'Receiver operating characteristic'))\n",
    "    \n",
    "    fig.append_trace(trace1,1,1)\n",
    "    fig.append_trace(trace2,1,2)\n",
    "    fig.append_trace(trace3,1,2)\n",
    "    \n",
    "    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n",
    "                         autosize = False,height = 900,width = 800,\n",
    "                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         margin = dict(b = 195))\n",
    "    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n",
    "    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n",
    "    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n",
    "                                        tickangle = 90))\n",
    "    py.iplot(fig)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_raw_data(model, device) :\n",
    "    X, Y = readXY('/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device)\n",
    "    acc = churn_validation(model, X,Y, X.columns)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processed, NON-anonymized data (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'Philips_B120N10_Baby_Monitor'\n",
    "filename = '/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device\n",
    "algorithm = RandomForestClassifier(n_jobs=-1,\n",
    "                                bootstrap=True,\n",
    "                                max_depth=10,\n",
    "                                min_samples_leaf=50,\n",
    "                                min_samples_split=50,\n",
    "                                n_estimators=60)\n",
    "model, acc =  train_model(filename, algorithm, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with pre-processed & anonymized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'Philips_B120N10_Baby_Monitor'\n",
    "algo = 'alex2'\n",
    "file = 'baby_k10_e1_lof'\n",
    "filename = '/data/dataprivacy/detection_of_IoT_botnet_attacks-anonymized/'+device+'/'+algo+'/'+file\n",
    "algorithm = RandomForestClassifier(n_jobs=-1,\n",
    "                                bootstrap=True,\n",
    "                                max_depth=10,\n",
    "                                min_samples_leaf=50,\n",
    "                                min_samples_split=50,\n",
    "                                n_estimators=60)\n",
    "model =  train_model(filename, algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917938760830626"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_with_raw_data(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [RandomForestClassifier(n_jobs=-1,\n",
    "                                bootstrap=True,\n",
    "                                max_depth=10,\n",
    "                                min_samples_leaf=50,\n",
    "                                min_samples_split=50,\n",
    "                                n_estimators=60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philips_B120N10_Baby_Monitor\n",
      "alex1b\n",
      "k10_e10_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k10_e1_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k20_e5_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k20_e1_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k50_e5_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k50_e1_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k20_e10_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k10_e5_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "k50_e10_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "alex2\n",
      "baby_k20_e5_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k10_e10_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k50_e5_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k50_e1_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k10_e1_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k20_e1_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby_k50_e10_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k20_e10_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "baby_k10_e5_lof\n",
      "0\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "1\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "2\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n",
      "3\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=50,\n",
      "                       n_estimators=60, n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "df_eval = pd.DataFrame(columns = ['accuracy'])\n",
    "#for device in ['SimpleHome_XCS7_1003_WHT_Security_Camera', 'Philips_B120N10_Baby_Monitor']:\n",
    "for device in ['Philips_B120N10_Baby_Monitor']:\n",
    "    print(device)\n",
    "    #original file beolvasas\n",
    "    files = ['benign_traffic', 'bashlite_combo', 'bashlite_junk', 'bashlite_scan', 'bashlite_tcp', 'bashlite_udp', 'mirai_ack', 'mirai_scan', 'mirai_syn', 'mirai_udp', 'mirai_udpplain']\n",
    "    #for algo_anon in ['alex2', 'attila1', 'attila2']:\n",
    "    for algo_anon in ['alex1b','alex2']:\n",
    "        print(algo_anon)\n",
    "        if algo_anon=='alex1b' or 'alex2':\n",
    "            df_original = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device, samp=True, lof=True)\n",
    "            df_original = df_original.dropna().reset_index(drop=True)\n",
    "        if algo_anon=='attila1' or 'attila2':\n",
    "            df_original = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device, samp=True)\n",
    "        for file in os.listdir('/data/dataprivacy/detection_of_IoT_botnet_attacks-anonymized/'+device+'/'+algo_anon):\n",
    "            if file.startswith('c') or file.startswith('k') or (file.startswith('b') and file.endswith('lof')):\n",
    "                print(file)\n",
    "                #file beolvasas\n",
    "                folder = '/data/dataprivacy/detection_of_IoT_botnet_attacks-anonymized/'+device+'/'+algo_anon+'/'+file\n",
    "                #df = read_file(folder)\n",
    "                #df = df.dropna().reset_index(drop=True)\n",
    "                ultim_best_scores = 0\n",
    "                df_ultim_best = pd.DataFrame()\n",
    "                for seed in [0,1,2,3]:\n",
    "                    print(seed)\n",
    "                    ultim_best_scores = 0\n",
    "                    df_ultim_best = pd.DataFrame()\n",
    "                    for algo in algos:\n",
    "                        print(algo)\n",
    "                        model, accuracy_train =  train_model(folder, algo, seed)\n",
    "                        if accuracy_train > ultim_best_scores:\n",
    "                            ultim_best_scores = accuracy_train\n",
    "                            best_model = model \n",
    "    \n",
    "                    ###\n",
    "                    df_eval.loc[device+'_'+algo_anon+'_'+file+'_'+str(seed)] = ultim_best_scores\n",
    "                    df_eval.loc[device+'_'+algo_anon+'_'+file+'_'+str(seed)+'_original'] = validate_with_raw_data(best_model, device)\n",
    "                    df_eval.to_csv('eval_total_class_alex_'+device+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_privacy(devices, algos):\n",
    "    df_avg_ident = pd.DataFrame(columns = ['avg_per_10'])\n",
    "    for device in devices:\n",
    "        print(device)\n",
    "        for algo in algos:\n",
    "            print(algo)\n",
    "            if algo=='attila1' or algo=='attila2':\n",
    "                df_original = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device, samp=True, lof=False)\n",
    "                print('attila')\n",
    "            else:\n",
    "                df_original = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device, samp=True, lof=True)\n",
    "                print('nemattila')\n",
    "            for file in os.listdir('/data/dataprivacy/detection_of_IoT_botnet_attacks-anonymized/'+device+'/'+algo):\n",
    "                #if file.startswith('c') or file.startswith('k'):\n",
    "                if file.endswith('lof'):\n",
    "                    print(file)\n",
    "                    if algo=='attila1' or algo=='attila2':\n",
    "                        if int(file[1])==3 or int(file[1])==5:\n",
    "                            num = int(file[1:2])\n",
    "                        else:\n",
    "                            num = int(file[1:3])\n",
    "                        print(num)\n",
    "                        df_original_cropped = crop_attila(df_original, num, device)\n",
    "                        df = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks-anonymized/'+device+'/'+algo+'/'+file)\n",
    "                        print(len(df_original_cropped))\n",
    "                        print(len(df))\n",
    "                        for j in df_original_cropped['label'].unique():\n",
    "                            if df_original_cropped[df_original_cropped['label'] == j].shape[0] != df[df['label'] == j].shape[0]:\n",
    "                                print('baaaj')\n",
    "                                print(j)\n",
    "                                print(df_original_cropped[df_original_cropped['label'] == j].shape[0])\n",
    "                                print(df[df['label'] == j].shape[0])\n",
    "                        X = df_original_cropped.iloc[:,:-1]\n",
    "                    else:\n",
    "                        df = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks-anonymized/'+device+'/'+algo+'/'+file)\n",
    "                        print(len(df_original))\n",
    "                        print(len(df))\n",
    "                        for j in df_original['label'].unique():\n",
    "                            if df_original[df_original['label'] == j].shape[0] != df[df['label'] == j].shape[0]:\n",
    "                                print('baaaaaaaaaaaaaaaaaaaaaaaaaaj')\n",
    "                                print(j)\n",
    "                                print(df_original[df_original['label'] == j].shape[0])\n",
    "                                print(df[df['label'] == j].shape[0])\n",
    "                        X = df_original.iloc[:,:-1]\n",
    "                    nbrs_original = NearestNeighbors(n_neighbors=11, algorithm='ball_tree').fit(X)\n",
    "                    distances_original, indices_original = nbrs_original.kneighbors(X)                    \n",
    "                    indices_original = indices_original[:,1:]\n",
    "                    inds = pd.isnull(df).any(1).to_numpy().nonzero()\n",
    "                    df = df.dropna().reset_index(drop=True)\n",
    "                    X = X.drop(inds[0]).reset_index(drop=True)\n",
    "                    Y = df.iloc[:,:-1]\n",
    "                    ##inds = pd.isnull(Y).any(1).nonzero()[0]\n",
    "                    #Y = Y.dropna().reset_index(drop=True)\n",
    "                    #X = X.drop(inds).reset_index(drop=True)\n",
    "                    print(len(X))\n",
    "                    print(len(Y))\n",
    "                    nbrs = NearestNeighbors(n_neighbors=11, algorithm='ball_tree').fit(Y)\n",
    "                    distances, indices = nbrs.kneighbors(Y)\n",
    "                    indices = indices[:,1:]\n",
    "                    identical = []\n",
    "                    for l in range(len(indices)):\n",
    "                        identical.append(len(set(indices_original[l]).intersection(indices[l])))\n",
    "                    df_avg_ident.loc[device+'_'+algo+'_'+file] = sum(identical)/len(identical)\n",
    "                    df_avg_ident.to_csv('eval_privacy_baby_alex2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
