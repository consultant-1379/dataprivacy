{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in /opt/conda/lib/python3.7/site-packages (0.8.27)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.7/site-packages (from hdbscan) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from hdbscan) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from hdbscan) (1.19.1)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/conda/lib/python3.7/site-packages (from hdbscan) (0.29.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.7/site-packages (from hdbscan) (0.24.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from hdbscan) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n",
      "Requirement already satisfied: fa_kit in /opt/conda/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fa_kit) (3.0.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fa_kit) (1.0.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fa_kit) (1.19.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fa_kit) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fa_kit) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fa_kit) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fa_kit) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fa_kit) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->fa_kit) (2020.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->fa_kit) (1.15.0)\n",
      "Requirement already satisfied: kneed in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.2 in /opt/conda/lib/python3.7/site-packages (from kneed) (1.19.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from kneed) (3.0.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from kneed) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->kneed) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->kneed) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->kneed) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->kneed) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->kneed) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "#!pip install scikit-learn==0.24.0\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN\n",
    "!pip install hdbscan\n",
    "!pip install fa_kit\n",
    "!pip install kneed\n",
    "from fa_kit.rotation import VarimaxRotatorPython\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "import seaborn as sns; sns.set();\n",
    "from seaborn import heatmap\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "import random as random\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import itertools as it\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in original data (wo anonymization)\n",
    "def read_file_o(dfs, device):\n",
    "    df = pd.DataFrame()\n",
    "    for i in dfs:\n",
    "        #df_new = pd.read_csv(i+'.csv')\n",
    "        #df_new = pd.read_csv('./Simple_home_1003_sec_cam/'+i+'.csv')\n",
    "        df_new = pd.read_csv('./'+device+'/'+i+'.csv')\n",
    "        df_new['label'] = i\n",
    "        df = pd.concat([df,df_new], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(folder, samp = False, lof=False):\n",
    "    df = pd.DataFrame()\n",
    "    path = folder+'/'\n",
    "    df_new = pd.read_csv(path+'benign_traffic.csv')\n",
    "    df_new['label'] = 'benign_traffic'\n",
    "    if samp==True:\n",
    "        df_new = df_new.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "    if lof==True:\n",
    "        #df_new = pd.DataFrame(scaler.fit_transform(df_new))\n",
    "        df_new = filter_lof(df_new)\n",
    "        #df_new = scaler.inverse_transform(df_new)\n",
    "    df = pd.concat([df,df_new], ignore_index=True)\n",
    "    \n",
    "    directory = os.fsencode(path+'gafgyt_attacks')\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".csv\"): \n",
    "            df_new = pd.read_csv(path+'gafgyt_attacks'+'/'+filename)\n",
    "            filename=filename[:-4] \n",
    "            df_new['label'] = 'gafgyt_'+filename\n",
    "            if samp==True:\n",
    "                df_new = df_new.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "            if lof==True:\n",
    "                #df_new = pd.DataFrame(scaler.fit_transform(df_new))\n",
    "                df_new = filter_lof(df_new)\n",
    "                #df_new = scaler.inverse_transform(df_new)\n",
    "            df = pd.concat([df,df_new], ignore_index=True)\n",
    "    directory = os.fsencode(path+'mirai_attacks')\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".csv\"): \n",
    "            df_new = pd.read_csv(path+'mirai_attacks'+'/'+filename)\n",
    "            filename=filename[:-4] \n",
    "            df_new['label'] = 'mirai_'+filename\n",
    "            if samp==True:\n",
    "                df_new = df_new.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "            if lof==True:\n",
    "                #df_new = pd.DataFrame(scaler.fit_transform(df_new))\n",
    "                df_new = filter_lof(df_new)\n",
    "                #df_new = scaler.inverse_transform(df_new)\n",
    "            df = pd.concat([df,df_new], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_func(rate, data):    \n",
    "    df_wo_label = data.drop(columns = ['label'])\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(df_wo_label)\n",
    "    pca = PCA(n_components=rate, random_state=0)\n",
    "    pca_comp = pca.fit(data_scaled).components_\n",
    "    return pca_comp, data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEts transform our original dataset to lower dimensional\n",
    "def decrease_dim(components, data_scaled):\n",
    "    varimax = VarimaxRotatorPython()\n",
    "    rotated_weights = varimax.rotate(components.T)\n",
    "    df_lowdim = pd.DataFrame(np.dot(data_scaled, rotated_weights))\n",
    "    #df_lowdim[\"label\"] = df[\"label\"]\n",
    "    rotated_data = np.dot(data_scaled, rotated_weights)\n",
    "    return df_lowdim, rotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def adj_rand(data):\n",
    "    tab = pd.crosstab(data['label'], data['cluster_label'])\n",
    "    df_tab = pd.DataFrame(tab)\n",
    "    a = list(df_tab.sum(axis=1))\n",
    "    b = list(df_tab.sum(axis=0))\n",
    "    n = df_tab.values.tolist()\n",
    "    sum_n = 0\n",
    "    for i in range(len(n)):\n",
    "        for j in range(len(n[0])):\n",
    "            sum_n += scipy.special.comb(n[i][j],2)\n",
    "    sum_a = 0\n",
    "    for k in range(len(a)):\n",
    "        sum_a += scipy.special.comb(a[k],2)\n",
    "    sum_b = 0\n",
    "    for l in range(len(b)):\n",
    "        sum_b += scipy.special.comb(b[l],2)\n",
    "    \n",
    "    ari = (sum_n - sum_a*sum_b/scipy.special.comb(sum(a),2))/(0.5*(sum_a+sum_b)-sum_a*sum_b/scipy.special.comb(sum(a),2))\n",
    "    return(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evall(df_cl):\n",
    "    sc = []\n",
    "    sc.append(metrics.rand_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    sc.append(adj_rand(df_cl))\n",
    "    sc.append(metrics.mutual_info_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    sc.append(metrics.adjusted_mutual_info_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    sc.append(metrics.homogeneity_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    sc.append(metrics.completeness_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    sc.append(metrics.v_measure_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    sc.append(metrics.fowlkes_mallows_score(df_cl['label'], df_cl['cluster_label']))\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(low_data, num_cl):\n",
    "    low_data_2 = low_data.copy()\n",
    "    kmeans = KMeans(init=\"random\",n_clusters=num_cl,n_init=10,max_iter=200,random_state=43)\n",
    "    kmeans.fit(low_data_2)\n",
    "    low_data_2[\"cluster_label\"] = kmeans.labels_\n",
    "    low_data_2[\"label\"] = df[\"label\"]\n",
    "    return low_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(low_data, num_cl):\n",
    "    low_data_2 = low_data.copy()\n",
    "    mini_kmeans = MiniBatchKMeans(init=\"random\",n_clusters=num_cl,n_init=10,max_iter=200,random_state=43)\n",
    "    mini_kmeans.fit(low_data_2)\n",
    "    low_data_2[\"cluster_label\"] = mini_kmeans.labels_\n",
    "    low_data_2[\"label\"] = df[\"label\"]\n",
    "    return low_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift(low_data):\n",
    "    low_data_2 = low_data.copy()\n",
    "    mean_sh = MeanShift().fit(low_data_2)\n",
    "    low_data_2[\"cluster_label\"] = mean_sh.labels_\n",
    "    low_data_2[\"label\"] = reduced_label\n",
    "    return low_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optics(low_data):\n",
    "    low_data_2 = low_data.copy()\n",
    "    opti = OPTICS().fit(low_data_2)\n",
    "    low_data_2[\"cluster_label\"] = opti.labels_\n",
    "    low_data_2[\"label\"] = reduced_label\n",
    "    return low_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(low_data, ep, min_samp):\n",
    "    low_data_2 = low_data.copy()\n",
    "    dbsc = DBSCAN(eps=ep, min_samples=min_samp).fit(low_data_2)\n",
    "    low_data_2[\"cluster_label\"] = dbsc.labels_\n",
    "    low_data_2[\"label\"] = reduced_label\n",
    "    return low_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def adj_rand_2(label_pred, label_true):\n",
    "    tab = pd.crosstab(label_true, label_pred)\n",
    "    df_tab = pd.DataFrame(tab)\n",
    "    a = list(df_tab.sum(axis=1))\n",
    "    b = list(df_tab.sum(axis=0))\n",
    "    n = df_tab.values.tolist()\n",
    "    sum_n = 0\n",
    "    for i in range(len(n)):\n",
    "        for j in range(len(n[0])):\n",
    "            sum_n += scipy.special.comb(n[i][j],2)\n",
    "    sum_a = 0\n",
    "    for k in range(len(a)):\n",
    "        sum_a += scipy.special.comb(a[k],2)\n",
    "    sum_b = 0\n",
    "    for l in range(len(b)):\n",
    "        sum_b += scipy.special.comb(b[l],2)\n",
    "    \n",
    "    ari = (sum_n - sum_a*sum_b/scipy.special.comb(sum(a),2))/(0.5*(sum_a+sum_b)-sum_a*sum_b/scipy.special.comb(sum(a),2))\n",
    "    return(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evall_2(label_pred, label_true):\n",
    "    sc = []\n",
    "    sc.append(metrics.rand_score(label_true, label_pred))\n",
    "    sc.append(adj_rand_2(label_true, label_pred))\n",
    "    sc.append(metrics.mutual_info_score(label_true, label_pred))\n",
    "    sc.append(metrics.adjusted_mutual_info_score(label_true, label_pred))\n",
    "    sc.append(metrics.homogeneity_score(label_true, label_pred))\n",
    "    sc.append(metrics.completeness_score(label_true, label_pred))\n",
    "    sc.append(metrics.v_measure_score(label_true, label_pred))\n",
    "    sc.append(metrics.fowlkes_mallows_score(label_true, label_pred))\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleHome_XCS7_1003_WHT_Security_Camera\n",
      "optics\n",
      "kmeans\n",
      "minibatch\n",
      "dbscan\n",
      "[0.8752314257142572, 0.4787536570722892, 1.523188557253452, 0.6927589680901312, 0.6817624784094604, 0.7056477879870281, 0.693499531428013, 0.5551218598933739]\n",
      "Philips_B120N10_Baby_Monitor\n",
      "optics\n",
      "kmeans\n",
      "minibatch\n",
      "dbscan\n",
      "[0.9154562245622456, 0.6018793261670413, 1.7763790787281346, 0.7857594010976108, 0.7896472173794143, 0.7822660136029327, 0.7859392856004022, 0.6500980071361933]\n"
     ]
    }
   ],
   "source": [
    "df_eval = pd.DataFrame(columns = ['rand_score', 'adjusted_rand_score', 'mutual_info_score', 'adjusted_mutual_info_score', \n",
    "                                  'homogeneity_score', 'completeness_score', 'v_measure_score', 'fowlkes_mallows_score'])\n",
    "#for device in ['SimpleHome_XCS7_1003_WHT_Security_Camera', 'Philips_B120N10_Baby_Monitor']:\n",
    "for device in ['SimpleHome_XCS7_1003_WHT_Security_Camera','Philips_B120N10_Baby_Monitor']:\n",
    "    print(device)\n",
    "    #original file beolvasas\n",
    "    files = ['benign_traffic', 'bashlite_combo', 'bashlite_junk', 'bashlite_scan', 'bashlite_tcp', 'bashlite_udp', 'mirai_ack', 'mirai_scan', 'mirai_syn', 'mirai_udp', 'mirai_udpplain']\n",
    "    #scaling\n",
    "    df_original = read_file('/data/dataprivacy/detection_of_IoT_botnet_attacks/'+device, samp=True)\n",
    "    df_reduced = df_original.sample(n=100000,replace=False)\n",
    "    reduced_label = df_reduced['label'].reset_index(drop=True)\n",
    "    comp, df_scaled = pca_func(0.7, df_reduced)\n",
    "    df_low, np_low = decrease_dim(comp, df_scaled)\n",
    "    ultim_best_scores = [0,0,0,0,0,0,0,0]\n",
    "    df_ultim_best = pd.DataFrame()                        \n",
    "    for algo in ['optics', 'kmeans', 'minibatch', 'dbscan']:\n",
    "        print(algo)\n",
    "        if algo is 'meanshift':\n",
    "            df_meanshift = mean_shift(df_low)\n",
    "            metrics_meanshift = evall(df_meanshift)\n",
    "            if metrics_meanshift[6]>ultim_best_scores[6]:\n",
    "                ultim_best_scores = metrics_meanshift\n",
    "                df_ultim_best = df_meanshift\n",
    "        if algo is 'optics':\n",
    "            df_optics = optics(df_low)\n",
    "            metrics_optics = evall(df_optics)\n",
    "            if metrics_optics[6]>ultim_best_scores[6]:\n",
    "                ultim_best_scores = metrics_optics\n",
    "                df_ultim_best = df_optics\n",
    "        if algo is 'kmeans':\n",
    "            eval_kmeans = [0,0,0,0,0,0,0,0]\n",
    "            df_best = pd.DataFrame()\n",
    "            for k in range(7,15):\n",
    "                df_kmeans = kmeans(df_low, k)\n",
    "                metrics_kmeans = evall(df_kmeans)\n",
    "                if metrics_kmeans[6]>eval_kmeans[6]:\n",
    "                    eval_kmeans=metrics_kmeans\n",
    "                    df_best_kmeans = df_kmeans\n",
    "            if eval_kmeans[6]>ultim_best_scores[6]:\n",
    "                ultim_best_scores = eval_kmeans\n",
    "                df_ultim_best = df_best_kmeans\n",
    "        if algo is 'minibatch':\n",
    "            eval_minibatch = [0,0,0,0,0,0,0,0]\n",
    "            df_best = pd.DataFrame()\n",
    "            for k in range(7,15):\n",
    "                df_minibatch = minibatch(df_low, k)\n",
    "                metrics_minibatch = evall(df_minibatch)\n",
    "                if metrics_minibatch[6]>eval_minibatch[6]:\n",
    "                    eval_minibatch=metrics_minibatch\n",
    "                    df_best_minibatch = df_minibatch\n",
    "                if eval_minibatch[6]>ultim_best_scores[6]:\n",
    "                    ultim_best_scores = eval_minibatch\n",
    "                    df_ultim_best = df_best_minibatch\n",
    "        if algo is 'dbscan':\n",
    "            eval_dbscan = [0,0,0,0,0,0,0,0]\n",
    "            df_best = pd.DataFrame()\n",
    "            for ep in [0.18,0.2,0.22,0.25,0.3,0.4,0.5]:\n",
    "                for min_sam in [3,5,8,10,12]:\n",
    "                    df_dbscan = dbscan(df_low, ep, min_sam)\n",
    "                    metrics_dbscan = evall(df_dbscan)\n",
    "                    if metrics_dbscan[6]>eval_dbscan[6]:\n",
    "                        eval_dbscan=metrics_dbscan\n",
    "                        df_best_dbscan = df_dbscan\n",
    "            if eval_dbscan[6]>ultim_best_scores[6]:\n",
    "                ultim_best_scores = eval_dbscan\n",
    "                df_ultim_best = df_best_dbscan\n",
    "    print(ultim_best_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
