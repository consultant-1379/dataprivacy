{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex's Algorithm #1\n",
    "\n",
    "Input: D-dimensional Eucledean Space Matrix of Real Valued Metric Space clustering use-cases/think through (this could be applied to other use cases as well e.g. anomaly detection w.o. starting with the filtering for these elements.\n",
    "\n",
    "Paramteres: \n",
    "\n",
    "K-anonymity level: K-anonymity\n",
    "\n",
    "Epsilon\n",
    "\n",
    "First filter out outlier rows from the data set, here one can use e.g. LOF or ABOD for this purpose or use winsorization column wise to cut off long tail distribution behavior (https://en.wikipedia.org/wiki/Local_outlier_factor , https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD2008.pdf , https://en.wikipedia.org/wiki/Winsorizing). /Or to be align with different use cases for e.g. anomaly detection, don't remove these elements./\n",
    "\n",
    "Run hierachical clustering on the Euclidean space data and save dendogram. \n",
    "(Here we would advise to use the linkage to keep simple with single or centroid linkage.\n",
    "\n",
    "\n",
    "If we denote the root with level 0 and the level of leafs with n, then j \\in \\{1,...,n\\} denotes the j'th level, and if we will also denote with i the i'th cluster at the j'th level, where i \\in \\{1,...,j\\}, with this notation we can define than c_{i,j} the cluster size at the i'th level for the j'th cluster. Now find i that statisfies\n",
    "\n",
    "\n",
    "    max_{i} min_{j} c_{i,j}>=k.\n",
    "\n",
    "\n",
    "Split the data into these clusters.\n",
    "\n",
    "\n",
    "For each cluster:\n",
    "\n",
    "\n",
    "For each column feature:\n",
    "\n",
    "\n",
    "        Calculate the centroid/mean value for the column per cluster and imputate it to the original data points per cluster + White Noise (white Noise should be different per cluster as defined below)\n",
    "        where White Noise = Laplace(0, (max(c_{i,j}[d]-min(c_{i,j}[d])/epsilon)\n",
    "        where c_{i,j}[d] denotes the set of feature/column \"d\" of the rows of the j'th cluster at the i'th level in the dendogram\n",
    "\n",
    "\n",
    "For categorical variables/non-numeric variables imputate cluster mode. /In case we need a common metric, the subscriber must normalize the space (e.g. with Min-Max Scaler) and should use a normalized Hamming- or Levenhstein-distance for them./\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MOdel selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Model hyper parameter tuning\n",
    "from sklearn import metrics\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.neighbors\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewValue(col, epsilon) :\n",
    "    # base = col.iloc[0]\n",
    "    base = numpy.mean(col)\n",
    "    r = max(col) - min(col)\n",
    "    return np.random.laplace(base, r/epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(k, tree, clusters) :\n",
    "    if tree.is_leaf():\n",
    "        raise Exception(\"Cluster contains only 1 element\")\n",
    "    left = tree.get_left()\n",
    "    right = tree.get_right()\n",
    "    if (left.get_count() >= k) and (right.get_count() >= k):\n",
    "        get_clusters(k, left, clusters)\n",
    "        get_clusters(k, right, clusters)\n",
    "    else:\n",
    "        clusters.append(tree.pre_order())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_transform(df, k, epsilon, lof=False) :\n",
    "    scaler = StandardScaler()\n",
    "    scaledDf = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    if lof:\n",
    "        scaledDf = filter_lof(scaledDf)\n",
    "\n",
    "    newDf = pd.DataFrame(index=scaledDf.index.copy(), columns=df.columns)\n",
    "\n",
    "        \n",
    "    linked = hierarchy.linkage(scaledDf, 'ward')\n",
    "    tree = hierarchy.to_tree(linked)\n",
    "    clusters = []\n",
    "    get_clusters(k, tree, clusters)\n",
    "    maxClusterSize = 0\n",
    "    for clusterIndexes in clusters:\n",
    "        if maxClusterSize < len(clusterIndexes):\n",
    "            maxClusterSize = len(clusterIndexes)\n",
    "        cluster = scaledDf.loc[clusterIndexes]\n",
    "        newCluster = cluster.apply(getNewValue, args=[epsilon])\n",
    "        for i in clusterIndexes : \n",
    "            newDf.loc[i] = newCluster\n",
    "\n",
    "    print(\"Max cluster size:\", maxClusterSize)\n",
    "    return pd.DataFrame(scaler.inverse_transform(newDf), columns=newDf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lof(df, k=20) :\n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    df2 = pd.DataFrame.copy(df)\n",
    "    df2[\"_lof\"] = lof.fit_predict(df2)\n",
    "    return df2[df2[\"_lof\"]>0].drop(columns=\"_lof\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main_transform(df, K, Epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 10, epsilon = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main_transform(df[0:200], 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers with LOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epsilon = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main_transform(filtered, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtered - main_transform(filtered, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"benign_traffic.csv\", \"gafgyt_attacks/combo.csv\", \"gafgyt_attacks/junk.csv\", \"gafgyt_attacks/scan.csv\", \"gafgyt_attacks/tcp.csv\", \"gafgyt_attacks/udp.csv\", \"mirai_attacks/ack.csv\", \"mirai_attacks/scan.csv\", \"mirai_attacks/syn.csv\", \"mirai_attacks/udp.csv\", \"mirai_attacks/udpplain.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(idir, odir, k, epsilon, lof=False) :\n",
    "    print(odir, k, epsilon, lof)\n",
    "    os.makedirs(odir)\n",
    "    os.makedirs(os.path.join(odir, \"mirai_attacks\"))\n",
    "    os.makedirs(os.path.join(odir, \"gafgyt_attacks\"))\n",
    "\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(os.path.join(idir, filename))\n",
    "        # df = df[0:200]\n",
    "        df = df.sample(frac=0.5, replace=False, random_state=42).reset_index(drop=True)\n",
    "        df2 = main_transform(df, k, epsilon, lof)\n",
    "        df2.to_csv(os.path.join(odir, filename), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baby Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "process_files(\"baby\", \"baby_k10_e1\", 10, 1)\n",
    "process_files(\"baby\", \"baby_k20_e1\", 20, 1)\n",
    "process_files(\"baby\", \"baby_k50_e1\", 50, 1)\n",
    "\n",
    "process_files(\"baby\", \"baby_k10_e5\", 10, 5)\n",
    "process_files(\"baby\", \"baby_k20_e5\", 20, 5)\n",
    "process_files(\"baby\", \"baby_k50_e5\", 50, 5)\n",
    "\n",
    "process_files(\"baby\", \"baby_k10_e10\", 10, 10)\n",
    "process_files(\"baby\", \"baby_k20_e10\", 20, 10)\n",
    "process_files(\"baby\", \"baby_k50_e10\", 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process_files(\"baby\", \"baby_k10_e1_lof\", 10, 1, True)\n",
    "process_files(\"baby\", \"baby_k20_e1_lof\", 20, 1, True)\n",
    "process_files(\"baby\", \"baby_k50_e1_lof\", 50, 1, True)\n",
    "\n",
    "process_files(\"baby\", \"baby_k10_e5_lof\", 10, 5, True)\n",
    "process_files(\"baby\", \"baby_k20_e5_lof\", 20, 5, True)\n",
    "process_files(\"baby\", \"baby_k50_e5_lof\", 50, 5, True)\n",
    "\n",
    "process_files(\"baby\", \"baby_k10_e10_lof\", 10, 10, True)\n",
    "process_files(\"baby\", \"baby_k20_e10_lof\", 20, 10, True)\n",
    "process_files(\"baby\", \"baby_k50_e10_lof\", 50, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleHome_XCS7_1003_WHT_Security_Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src = \"/data/dataprivacy/detection_of_IoT_botnet_attacks/SimpleHome_XCS7_1003_WHT_Security_Camera\"\n",
    "\n",
    "src = \"/data/dataprivacy/detection_of_IoT_botnet_attacks/Philips_B120N10_Baby_Monitor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dataprivacy/detection_of_IoT_botnet_attacks/Philips_B120N10_Baby_Monitor\n",
      "k10_e1_lof 10 1 True\n",
      "benign_traffic.csv\n"
     ]
    }
   ],
   "source": [
    "print(src)\n",
    "process_files(src, \"k10_e1_lof\", 10, 1, True)\n",
    "process_files(src, \"k20_e1_lof\", 20, 1, True)\n",
    "process_files(src, \"k50_e1_lof\", 50, 1, True)\n",
    "\n",
    "process_files(src, \"k10_e5_lof\", 10, 5, True)\n",
    "process_files(src, \"k20_e5_lof\", 20, 5, True)\n",
    "process_files(src, \"k50_e5_lof\", 50, 5, True)\n",
    "\n",
    "process_files(src, \"k10_e10_lof\", 10, 10, True)\n",
    "process_files(src, \"k20_e10_lof\", 20, 10, True)\n",
    "process_files(src, \"k50_e10_lof\", 50, 10, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
